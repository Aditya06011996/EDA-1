{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969c2581-08c4-43e5-b9ef-477aa49da59d",
   "metadata": {},
   "source": [
    "Q1. What are the key features of the wine quality data set? Discuss the importance of each feature in\n",
    "predicting the quality of wine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1a7dd-f007-4fa5-9ef4-43d97440e0b5",
   "metadata": {},
   "source": [
    "Ans - The wine quality dataset typically refers to two datasets: one for red wine and one for white wine. These datasets contain various features that are used to predict the quality of wine. Let's discuss the key features and their importance in predicting wine quality:\n",
    "\n",
    "1. Fixed Acidity:\n",
    "   - Importance: Fixed acidity represents the total concentration of acids (non-volatile) in the wine, mainly tartaric acid. It plays a role in the wine's taste and pH level.\n",
    "   \n",
    "2. Volatile Acidity:\n",
    "   - Importance: Volatile acidity refers to the concentration of volatile acids, primarily acetic acid, which can negatively affect the wine's aroma and taste. High levels of volatile acidity are generally undesirable.\n",
    "\n",
    "3. Citric Acid:\n",
    "   - Importance: Citric acid is a weak organic acid that can contribute to the wine's freshness and flavor. It adds a citrusy note to the wine and can enhance its overall balance.\n",
    "\n",
    "4. Residual Sugar:\n",
    "   - Importance: Residual sugar is the amount of sugar remaining in the wine after fermentation. It affects the wine's sweetness and can balance out the acidity, influencing the perceived taste.\n",
    "\n",
    "5. Chlorides:\n",
    "   - Importance: Chlorides represent the concentration of salts in the wine, mainly sodium chloride. They can influence the wine's taste and are typically kept at low levels to avoid a salty or briny flavor.\n",
    "\n",
    "6. Free Sulfur Dioxide:\n",
    "   - Importance: Free sulfur dioxide is used as a preservative in wine. It helps prevent spoilage by inhibiting microbial growth and oxidation. The level of free sulfur dioxide can impact wine stability and aging potential.\n",
    "\n",
    "7. Total Sulfur Dioxide:\n",
    "   - Importance: Total sulfur dioxide includes both free and bound sulfur dioxide. It is an important parameter for wine preservation and can also affect wine quality and aroma.\n",
    "\n",
    "8. Density:\n",
    "   - Importance: Density is a measure of the wine's mass per unit volume. It can be influenced by the concentration of alcohol and sugar, giving insights into the wine's body and potential sweetness.\n",
    "\n",
    "9. pH:\n",
    "   - Importance: pH measures the acidity or alkalinity of the wine. It affects the wine's taste, stability, and the perception of other flavors. Wines with the right pH tend to be more balanced.\n",
    "\n",
    "10. Sulphates:\n",
    "    - Importance: Sulphates, often in the form of potassium sulphate, can act as a nutrient for yeast during fermentation. They may contribute to the wine's aroma and stability.\n",
    "\n",
    "11. Alcohol:\n",
    "    - Importance: Alcohol content significantly impacts the wine's body, flavor, and overall balance. It is an essential factor in determining wine style and quality.\n",
    "\n",
    "12. Quality (Target Variable):\n",
    "    - Importance: This is the variable we want to predict. Wine quality is usually rated on a scale, with higher values indicating better quality. It is the ultimate goal of the analysis, and all other features are considered in relation to it.\n",
    "\n",
    "Each of these features can play a crucial role in predicting wine quality, as they collectively influence the wine's sensory characteristics, stability, and overall balance. By analyzing these features, wine producers and enthusiasts can gain insights into the factors that contribute to wine quality and make informed decisions about production and consumption. Machine learning models can also be trained on this data to predict wine quality based on these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be42865a-1d6d-4d2a-aca4-1c98c1b97bc3",
   "metadata": {},
   "source": [
    "Q2. How did you handle missing data in the wine quality data set during the feature engineering process?\n",
    "Discuss the advantages and disadvantages of different imputation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73959c4-2a90-48fd-b794-e346eeb8ee69",
   "metadata": {},
   "source": [
    "Ans - Handling missing data is an essential step in the feature engineering process when working with datasets like the wine quality dataset. There are several techniques for dealing with missing data, each with its own advantages and disadvantages. Let's discuss some common imputation techniques and their pros and cons:\n",
    "\n",
    "1. **Deletion of Missing Data (Listwise Deletion):**\n",
    "   - **Advantages:**\n",
    "     - Simple and straightforward.\n",
    "     - Does not introduce bias into the dataset.\n",
    "   - **Disadvantages:**\n",
    "     - Can lead to a loss of valuable information, especially if a large portion of the data is missing.\n",
    "     - Reduces the effective sample size, which can affect statistical power.\n",
    "\n",
    "2. **Mean/Median Imputation:**\n",
    "   - **Advantages:**\n",
    "     - Simple and quick.\n",
    "     - Preserves the original data distribution when using the median.\n",
    "   - **Disadvantages:**\n",
    "     - May not be suitable for variables with skewed or non-normal distributions.\n",
    "     - Can introduce bias if the data is not missing completely at random (MCAR).\n",
    "\n",
    "3. **Mode Imputation (for Categorical Data):**\n",
    "   - **Advantages:**\n",
    "     - Suitable for categorical data.\n",
    "     - Preserves the most frequent category.\n",
    "   - **Disadvantages:**\n",
    "     - May not be appropriate if the mode does not represent the true underlying distribution.\n",
    "     - Like mean/median imputation, it can introduce bias if data is not MCAR.\n",
    "\n",
    "4. **Regression Imputation:**\n",
    "   - **Advantages:**\n",
    "     - Can provide more accurate imputations by modeling relationships between variables.\n",
    "     - Preserves the relationships between variables.\n",
    "   - **Disadvantages:**\n",
    "     - Requires more computational resources and time.\n",
    "     - Assumes a linear relationship between variables, which may not hold in all cases.\n",
    "\n",
    "5. **K-Nearest Neighbors (KNN) Imputation:**\n",
    "   - **Advantages:**\n",
    "     - Imputes missing values by considering the similarity between data points.\n",
    "     - Can handle both numerical and categorical data.\n",
    "   - **Disadvantages:**\n",
    "     - Computationally intensive, especially with large datasets.\n",
    "     - Sensitive to the choice of the number of neighbors (k).\n",
    "\n",
    "6. **Multiple Imputation:**\n",
    "   - **Advantages:**\n",
    "     - Provides a robust approach by generating multiple imputed datasets.\n",
    "     - Accounts for uncertainty in imputations.\n",
    "   - **Disadvantages:**\n",
    "     - Complex and computationally expensive.\n",
    "     - Requires careful consideration of the imputation model.\n",
    "\n",
    "7. **Domain-Specific Imputation:**\n",
    "   - **Advantages:**\n",
    "     - Tailored to the specific characteristics of the dataset.\n",
    "     - Can utilize domain knowledge to make informed imputations.\n",
    "   - **Disadvantages:**\n",
    "     - May require expertise and manual intervention.\n",
    "     - Could introduce bias if domain knowledge is incomplete or incorrect.\n",
    "\n",
    "The choice of imputation technique depends on the nature of the missing data, the dataset size, and the specific goals of the analysis. For the wine quality dataset, where the data is likely related to sensory and chemical characteristics, it may be essential to carefully consider the imputation method to preserve the quality of the data and the interpretability of results.\n",
    "\n",
    "Additionally, it's crucial to assess the missing data mechanism (MCAR, MAR, or MNAR) and perform sensitivity analysis to understand the potential impact of different imputation methods on the analysis results. Multiple imputation techniques, such as Multiple Imputation by Chained Equations (MICE), can be particularly useful when dealing with complex datasets with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff0d293-279e-4705-b3ca-b1e870c4a984",
   "metadata": {},
   "source": [
    "Q3. What are the key factors that affect students' performance in exams? How would you go about\n",
    "analyzing these factors using statistical techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae80844-9426-4697-be68-7cd893040a62",
   "metadata": {},
   "source": [
    "Ans - Students' performance in exams can be influenced by a wide range of factors, both academic and non-academic. Analyzing these factors using statistical techniques can help identify the key drivers of academic success. Here are some key factors that can affect students' exam performance:\n",
    "\n",
    "1. **Study Habits and Time Management:**\n",
    "   - Analytical Approach: Collect data on students' study habits, including study duration, study materials used, and study environment. Analyze how these habits correlate with exam scores using regression analysis.\n",
    "\n",
    "2. **Attendance and Class Participation:**\n",
    "   - Analytical Approach: Gather attendance and participation records and examine their relationship with exam performance through correlation analysis.\n",
    "\n",
    "3. **Prior Knowledge and Preparation:**\n",
    "   - Analytical Approach: Evaluate students' pre-existing knowledge in the subject area, such as their performance in prerequisite courses or pre-tests. Analyze how prior knowledge correlates with exam scores.\n",
    "\n",
    "4. **Teacher Quality and Teaching Methods:**\n",
    "   - Analytical Approach: Gather data on teacher effectiveness, teaching methods, and classroom environment. Use regression analysis to determine if these factors impact student performance.\n",
    "\n",
    "5. **Motivation and Engagement:**\n",
    "   - Analytical Approach: Administer surveys or questionnaires to assess students' motivation levels and engagement in the course. Analyze survey data using statistical techniques like factor analysis or structural equation modeling to identify underlying factors affecting motivation and engagement.\n",
    "\n",
    "6. **Socioeconomic Background:**\n",
    "   - Analytical Approach: Collect data on students' socioeconomic status, including parental income and education. Use regression analysis to investigate the influence of socioeconomic factors on exam scores.\n",
    "\n",
    "7. **Health and Well-being:**\n",
    "   - Analytical Approach: Analyze the relationship between students' physical and mental health and their exam performance using regression analysis or logistic regression for binary outcomes (e.g., pass/fail).\n",
    "\n",
    "8. **Peer Group and Social Support:**\n",
    "   - Analytical Approach: Explore how peer interactions and social support networks impact students' study habits and performance through surveys and regression analysis.\n",
    "\n",
    "9. **Test Anxiety and Stress Levels:**\n",
    "   - Analytical Approach: Administer surveys to assess students' levels of test anxiety and stress. Analyze the data using correlation analysis to determine if these factors are negatively correlated with exam scores.\n",
    "\n",
    "10. **Technology Usage:**\n",
    "    - Analytical Approach: Investigate how students' use of technology for learning (e.g., online resources, educational apps) relates to their exam performance through regression analysis.\n",
    "\n",
    "Statistical techniques that can be employed to analyze these factors include:\n",
    "\n",
    "- **Descriptive Statistics:** Begin with summary statistics to understand the distribution of exam scores and other variables of interest.\n",
    "\n",
    "- **Correlation Analysis:** Assess the strength and direction of relationships between variables using correlation coefficients (e.g., Pearson's correlation).\n",
    "\n",
    "- **Regression Analysis:** Perform multiple regression analysis to identify significant predictors of exam performance while controlling for other variables.\n",
    "\n",
    "- **Hypothesis Testing:** Conduct hypothesis tests to determine if certain factors have a statistically significant impact on exam scores.\n",
    "\n",
    "- **Factor Analysis:** Use factor analysis to identify latent constructs (e.g., motivation, study habits) that may influence performance.\n",
    "\n",
    "- **Structural Equation Modeling (SEM):** Employ SEM to examine complex relationships between multiple factors simultaneously.\n",
    "\n",
    "- **Logistic Regression:** For binary outcomes like pass/fail exams, logistic regression can be used to model the likelihood of passing as a function of predictor variables.\n",
    "\n",
    "- **Survey Analysis:** Analyze survey data using techniques like factor analysis, principal component analysis, or regression to extract meaningful insights.\n",
    "\n",
    "It's essential to collect high-quality data, consider potential confounding variables, and use appropriate statistical techniques to draw valid conclusions about the factors influencing students' exam performance. Additionally, a well-designed research study may involve longitudinal data collection to assess changes in performance over time and the effectiveness of interventions aimed at improving academic outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acd7af6-378d-4a56-9e91-4540890d0b01",
   "metadata": {},
   "source": [
    "Q4. Describe the process of feature engineering in the context of the student performance data set. How\n",
    "did you select and transform the variables for your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e898f4a7-2a7b-447a-9e3d-39317ed6f951",
   "metadata": {},
   "source": [
    "Ans - Feature engineering is a critical step in the data preprocessing phase when working with a student performance dataset. The goal is to select, create, or transform variables (features) that are most relevant for building a predictive model or gaining insights from the data. Here's a general process for feature engineering in the context of a student performance dataset:\n",
    "\n",
    "1. **Data Collection and Exploration:**\n",
    "   - Begin by gathering the student performance dataset, which typically includes variables such as exam scores, demographic information, study habits, attendance records, and more.\n",
    "   - Explore the dataset to understand the types of variables, their distributions, and potential missing values.\n",
    "\n",
    "2. **Feature Selection:**\n",
    "   - Identify the target variable: In this case, it might be a measure of student performance, such as final exam scores or pass/fail status.\n",
    "   - Select potential predictor variables: Consider variables that may influence student performance, such as demographic information (e.g., age, gender), socioeconomic factors, study-related variables, and attendance.\n",
    "   - Use domain knowledge or statistical techniques (e.g., correlation analysis) to narrow down the list of potential predictors.\n",
    "\n",
    "3. **Handling Missing Data:**\n",
    "   - Address missing data in selected variables using appropriate imputation techniques as discussed earlier (mean, median, regression imputation, etc.).\n",
    "\n",
    "4. **Data Transformation:**\n",
    "   - Convert categorical variables: If the dataset includes categorical variables (e.g., gender), encode them into numerical values using techniques like one-hot encoding or label encoding.\n",
    "   - Scaling and normalization: Standardize numerical variables to have zero mean and unit variance (z-score normalization) to ensure that variables are on a similar scale. This is especially important for algorithms sensitive to feature scaling, such as K-nearest neighbors or gradient-based models.\n",
    "\n",
    "5. **Feature Creation:**\n",
    "   - Generate new features based on domain knowledge: Create composite features or interaction terms that capture meaningful relationships. For example, you could create a \"study hours per week\" feature by combining \"study time\" and \"travel time\" variables.\n",
    "   - Create binary features: Convert continuous variables into binary categories, such as \"high/low attendance\" or \"high/low socioeconomic status,\" based on predefined thresholds.\n",
    "\n",
    "6. **Handling Outliers:**\n",
    "   - Identify and deal with outliers in numerical variables, either by capping/extending values or applying transformation techniques (e.g., log transformation) to mitigate their impact on the model.\n",
    "\n",
    "7. **Feature Scaling:**\n",
    "   - Ensure that all features are on a similar scale to prevent some variables from dominating the modeling process. This step may involve additional scaling or standardization.\n",
    "\n",
    "8. **Feature Engineering Iteration:**\n",
    "   - Perform iterative feature selection and engineering by building initial models and evaluating feature importance or significance.\n",
    "   - Remove irrelevant or redundant features that do not contribute significantly to the predictive power of the model.\n",
    "\n",
    "9. **Validation and Model Building:**\n",
    "   - Split the dataset into training and validation sets (e.g., using cross-validation) to assess model performance.\n",
    "   - Build predictive models (e.g., regression, classification) using the selected and engineered features.\n",
    "   - Evaluate model performance using appropriate metrics (e.g., mean squared error for regression, accuracy for classification).\n",
    "\n",
    "10. **Feature Importance Analysis:**\n",
    "    - Analyze feature importance scores provided by some models (e.g., decision trees, random forests) to further refine feature selection.\n",
    "\n",
    "11. **Iterative Refinement:**\n",
    "    - Continue to iterate through steps 6 to 10, refining the feature set and model until satisfactory performance is achieved.\n",
    "\n",
    "The specific variables to select and transform will depend on the goals of your analysis. For example, if you aim to predict student performance, you may prioritize variables related to study habits, attendance, and prior academic performance. However, if your goal is to understand the factors influencing student performance, you might explore a broader range of variables, including demographic and socioeconomic factors.\n",
    "\n",
    "Ultimately, feature engineering is an iterative process that combines domain knowledge, data exploration, and modeling techniques to create a feature set that maximizes the model's predictive power or insights gained from the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50505b8-f906-475e-aa5a-8df5d6faf6bf",
   "metadata": {},
   "source": [
    "Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution\n",
    "of each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to\n",
    "these features to improve normality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b9336-ddb5-4eb1-a767-953cfbdcd39a",
   "metadata": {},
   "source": [
    "Ans - I cannot directly load or analyze datasets, but I can guide you through the process of performing exploratory data analysis (EDA) on the wine quality dataset to identify the distribution of each feature and address non-normality if observed. You can use tools like Python with libraries like pandas, matplotlib, and seaborn to conduct EDA. Here's a general outline of the process:\n",
    "\n",
    "1. **Load the Wine Quality Dataset:**\n",
    "   - Import the necessary libraries and load the wine quality dataset, which typically includes red wine and white wine datasets.\n",
    "\n",
    "2. **Explore the Data:**\n",
    "   - Examine the dataset's structure, dimensions, and the first few rows to get a sense of its content.\n",
    "\n",
    "3. **Summary Statistics:**\n",
    "   - Calculate summary statistics for each feature, including mean, median, standard deviation, skewness, and kurtosis. These statistics can provide insights into the distribution of the data.\n",
    "\n",
    "4. **Data Visualization:**\n",
    "   - Create visualizations to understand the distribution of each feature. Common plots include histograms, box plots, and density plots.\n",
    "\n",
    "Now, let's discuss how to identify non-normality and potential transformations for non-normal features:\n",
    "\n",
    "**Identifying Non-Normality:**\n",
    "   - **Histograms:** Plot histograms for each feature to visualize their distribution. If a feature's histogram is skewed (not symmetric) or has multiple peaks, it may indicate non-normality.\n",
    "   - **Q-Q Plots:** Quantile-quantile (Q-Q) plots can be used to compare the distribution of a feature against a theoretical normal distribution. Deviations from a straight line in the Q-Q plot suggest non-normality.\n",
    "   - **Shapiro-Wilk Test:** Perform the Shapiro-Wilk normality test for each feature. A low p-value indicates departure from normality.\n",
    "\n",
    "**Potential Transformations to Improve Normality:**\n",
    "   - **Log Transformation:** Apply a logarithmic transformation (e.g., natural logarithm) to reduce right skewness in positively skewed data. This is useful for features with a long right tail.\n",
    "   - **Square Root Transformation:** Use the square root transformation to reduce skewness and variance for data that is positively skewed.\n",
    "   - **Box-Cox Transformation:** The Box-Cox transformation is a family of power transformations that can make the data more normal. It includes logarithmic and square root transformations as special cases.\n",
    "   - **Inverse Transformation:** The inverse transformation (1/x) can be applied to data that is negatively skewed, making it closer to a normal distribution.\n",
    "\n",
    "For example, if the \"Total Sulfur Dioxide\" feature in the wine dataset exhibits right skewness, you could apply a log transformation to make it more normal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c5179-9c70-4d94-8c7d-6796b7ee0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame and 'total_sulfur_dioxide' is the column you want to transform\n",
    "df['total_sulfur_dioxide'] = np.log(df['total_sulfur_dioxide'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de5df2a-35c9-40b7-95fe-3d44b0967701",
   "metadata": {},
   "source": [
    "After applying transformations, it's essential to re-assess the normality of the features using the same techniques mentioned above and, if necessary, further refine the transformations to achieve normality. Keep in mind that the choice of transformation should be based on the specific characteristics of the data and the goals of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a704fd-8bba-4dfe-884c-7ec4662fd2f9",
   "metadata": {},
   "source": [
    "Q6. Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of\n",
    "features. What is the minimum number of principal components required to explain 90% of the variance in\n",
    "the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f61673-1608-4d46-bd44-e30f821bdd44",
   "metadata": {},
   "source": [
    "Ans - Principal Component Analysis (PCA) is a dimensionality reduction technique that can be used to reduce the number of features in a dataset while retaining most of the variance. To determine the minimum number of principal components required to explain 90% of the variance in the wine quality dataset, you can follow these steps in Python using libraries like NumPy and scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189dfaf-663a-414b-ba4f-01d941c91d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the wine quality dataset (either red or white wine dataset)\n",
    "# For this example, let's assume you have loaded it into a DataFrame called 'wine_df'\n",
    "\n",
    "# Separate the target variable (quality) from the predictors\n",
    "X = wine_df.drop('quality', axis=1)\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate the explained variance ratio for each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Determine the number of components required to explain 90% of the variance\n",
    "num_components_90_percent_variance = np.argmax(cumulative_explained_variance >= 0.90) + 1\n",
    "\n",
    "print(f\"Number of components to explain 90% of variance: {num_components_90_percent_variance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7df87-c416-4671-9411-8a2de2c69f36",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this code:\n",
    "\n",
    "1. We standardize the data to have zero mean and unit variance using `StandardScaler` to ensure that features with different scales do not dominate the PCA.\n",
    "\n",
    "2. We perform PCA on the standardized data using `PCA()` from scikit-learn.\n",
    "\n",
    "3. We calculate the explained variance ratio for each principal component, which represents the proportion of total variance explained by each component.\n",
    "\n",
    "4. We compute the cumulative explained variance by cumulatively summing the explained variance ratios.\n",
    "\n",
    "5. We find the number of principal components required to explain at least 90% of the variance by checking where the cumulative explained variance exceeds 90%.\n",
    "\n",
    "The variable `num_components_90_percent_variance` will give you the minimum number of principal components needed to explain 90% of the variance in the wine quality dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c63ced9-ae8c-432b-b7d7-d7e1f2670732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63eb0d94-7ff9-47ba-bf68-67f8810bfcd2",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00fa42-436b-4ae7-9b2f-19cecb97a39b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
